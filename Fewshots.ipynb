{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=0\n",
    "# Mock function to simulate API calls\n",
    "def promptgroq_fewshots(csv, start_idx, chunksize, model, num_samples, initial_examples):\n",
    "    global appliances\n",
    "    answers = {'timestamps': [], 'app_predictions': {}}\n",
    "    for app in appliances:\n",
    "        answers['app_predictions'][app] = []\n",
    "    attempts = 40\n",
    "    predictions = []\n",
    "    global api_keyover, client, i\n",
    "\n",
    "    initial_examples_str = \", \".join(\n",
    "    [f\"Timestamp: {row['timestamp']}, Total Power: {row['total_power']}, Fridge Power: {row['fridge']},Dish washer Power: {row['dish washer']}\"\n",
    "     for idx, row in initial_examples.iterrows()]\n",
    ")\n",
    "\n",
    "\n",
    "    def get_prediction(run, csv=csv, start_idx=start_idx, chunksize=chunksize, model=model):\n",
    "        global i, api_keyover, client\n",
    "        while i < attempts:\n",
    "            try:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model='llama3-8b-8192',\n",
    "                    messages=[\n",
    "                        \n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"\"\"You are a helpful assistant that performs time series predictions for NILM (Non-Intrusive Load Monitoring). The user will provide an aggregate data sequence of power consumption and timestamps with a few examples of power consumed by fridge and dish washer, and you will predict the power consumption of the fridge corresponding to each timestamp in the data. The sequence is represented by decimal strings separated by commas for power values and HH:MM format for timestamps \n",
    "                                            DO NOT CHANGE THE TIMESTAMPS KEEP THEM SAME AS IN THE DATA. USE EXAMPLE STRING PROVIDED AS A TRAINING SET\n",
    "                                            Also keep in mind the other appliances while predicting.\n",
    "                                            Appliance Information:\n",
    "                                            Low power consuming appliances - Lighting, Electronics, Fridge\n",
    "                                            Moderate power consuming appliances - Bathroom GFI, Dishwasher, Microwave, Kitchen Outlets\n",
    "                                            High power consuming appliances - Washer Dryer   \n",
    "                                        \"\"\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"\"\"Predict the fridge and dish washer power consumption for the given time series data for the next {chunksize} timestamps by using the examples provided. Each timestamp in the input should have a corresponding output. Return your reply in the following format:\n",
    "                                            [timestamp 0, prediction 0, timestamp 1, prediction 1, ..., timestamp {chunksize - 1}, prediction {chunksize - 1}] - the total length should be exactly {chunksize * 2}, starting from timestamp {start_idx} until the timestamp {start_idx + chunksize - 1}. Do not include any additional text or explanations.\n",
    "                                            \n",
    "                                            ### Initial Examples\n",
    "                                            {initial_examples_str}\n",
    "                                            \n",
    "                                            ### Time Series Data\n",
    "                                            {csv}\n",
    "                                            \"\"\"\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=1,\n",
    "                    top_p=1,\n",
    "                    stream=False,\n",
    "                    stop=None,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print('API key limit reached', e)\n",
    "                api_keyover += 1\n",
    "                client = Groq(api_key=api_keys[api_keyover % 4])\n",
    "                continue\n",
    "            try:\n",
    "                    answer = completion.choices[0].message.content.strip()\n",
    "                    predictions_list = [item.strip() for item in answer.split(',') if item.strip()]\n",
    "\n",
    "                    pred_dict_app = {}\n",
    "                    \n",
    "                    pred_timestamps = predictions_list[0::len(appliances)+1]\n",
    "                    for i, app in enumerate(appliances):\n",
    "                        pred_dict_app[app] = predictions_list[i+1::len(appliances)+1]\n",
    "\n",
    "                    pred_cleaned = {} \n",
    "                    for app in appliances:\n",
    "                        pred_values_cleaned = []\n",
    "                        for value in pred_dict_app[app]:\n",
    "                            try:\n",
    "                                pred_values_cleaned.append(float(value))\n",
    "                            except ValueError:\n",
    "                                float_part = ''.join([char for char in value if char.isdigit() or char == '.' or char == '-'])\n",
    "                                pred_values_cleaned.append(float(float_part))\n",
    "                        pred_cleaned[app] = pred_values_cleaned\n",
    "                    \n",
    "                    pred_df = pd.DataFrame({\n",
    "                        'timestamp': [ts.strip() for ts in pred_timestamps]\n",
    "                    })\n",
    "\n",
    "                    for appliance, predictions in pred_cleaned.items():\n",
    "                        pred_df[appliance] = predictions\n",
    "\n",
    "                    if len(pred_timestamps) == chunksize:\n",
    "                        return pred_df\n",
    "                    else:\n",
    "                        i += 1\n",
    "            except Exception as e:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        return None\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_run = {executor.submit(get_prediction, run): run for run in range(num_samples)}\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_run)):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                predictions.append(result)\n",
    "\n",
    "    if len(predictions) < num_samples:\n",
    "        return promptgroq(csv, start_idx, chunksize, model, num_samples)\n",
    "\n",
    "    for i in range(chunksize):\n",
    "        for app in appliances:\n",
    "            sum = 0\n",
    "            for j in range(num_samples):\n",
    "                sum += predictions[j][app][i]\n",
    "            answers['app_predictions'][app].append(sum / num_samples)\n",
    "        answers['timestamps'].append(predictions[0]['timestamp'][i])\n",
    "        \n",
    "    return answers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = 3\n",
    "train_df = result_df[:hours*60]\n",
    "test_df = result_df[hours*60:]\n",
    "pred_df_list = []\n",
    "\n",
    "# Select a small set of initial examples (e.g., first 5 rows)\n",
    "initial_examples = train_df[['timestamp', 'total_power', 'fridge_power']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinationstorun=[[0,15]]\n",
    "model = 'llama3-70b-8192'\n",
    "pred_df_list = []\n",
    "# Run the loop\n",
    "for start_idx, chunksize in combinationstorun:\n",
    "    for start in tqdm(range(0, len(result_df) - (start_idx + chunksize), 15)):\n",
    "        current_chunk = result_df.iloc[start:start+start_idx+chunksize]\n",
    "        input_string = current_chunk[['timestamp', 'total_power']].to_csv(index=False, header=False)\n",
    "        input_string = input_string.replace(\" \", \"\").replace('\\n', ',').replace(',', ' , ')\n",
    "        answer = promptgroq(input_string, start + start_idx, chunksize, model, num_samples=2)\n",
    "        predictions = answer['app_predictions']\n",
    "        timestamps = answer['timestamps']\n",
    "        predicted_df = pd.DataFrame({'Timestamp': timestamps})\n",
    "        for appliance, predictions in predictions.items():\n",
    "            predicted_df[appliance] = predictions\n",
    "        print(predicted_df)\n",
    "        pred_df_list.append(predicted_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nilmtk-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
